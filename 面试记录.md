# 面试准备

## 影刀erp实施工程师面试记录
1. 日志的收集和处理，通过什么方式拿到日志，然后怎样进行处理？胡扯的
2. 跨域操作，前端后端怎样完成？胡扯的
3. 每天的新的接口是怎样发现的？胡扯的
4. 怎样进行端口扫描？胡扯的
5. redis和mysql的区别，redis的数据类型有哪些？
6. Linux的基本命令，查看日志（前几行后几行），守护进程的启动方式

答案：

1. 1
2. 2
3. 3
4. 4
5. 

## 海康威视机器人测试平台开发
1. 前端vue2组件和脚手架介绍？
2. django前后端分离，如何确认身份信息？
3. 项目中哪些部分使用了异步？介绍一下异步任务
4. 介绍一下celery
3. django框架出了写脚本，本身做了什么？（我猜是django框架的作用和优势吧

## 华为

### 一面

1. 项目介绍
2. 项目相关：日志机器人怎么部署的；怎么消费kafka日志；怎么样才可以知道性能到达了瓶颈；消费日志的性能有瓶颈怎样去扩展；
3. python相关：列表和元组的区别；怎样实现协程（项目用到了）；装饰器是什么，有什么特点；线程安全怎么实现
4. redis：redis用来做什么（项目相关）（经常需要用到的数据存redis）
5. 手撕代码：leetcode 3 无重复字符的最长子串

**感受**：体验非常好，面试官问的问题很有切入点，根据项目内容层层深入。

### 二面

1. 手撕代码：leetcode 1190. 反转每对括号间的子串。一上来就写算法题真的傻逼。
2. 项目相关：基本没问什么，就让我讲了一下项目内容，我做了哪些部分，然后什么都没问。
3. python：while for else用法和特性（我不知道……就离谱）；try except finally 用法return值问题
4. mysql：什么时候会发生死锁；锁的级别是什么；锁的类型有哪些
5. 业务沟通：项目相关，讲了一下推进业务整改
6. 项目所需都是自己在实习之后自己学习的吗？
7. 有什么难点，怎么解决的：讲了一下django和dbv数据库连接被服务端断链的bug解决

**体验**：真的傻逼，先写代码就算了，后面提问也不解释。讲项目讲完一遍也没有任何问题。真的无语

夏晨问题：
1. 日志一般包含的内容？
2. filebeat的部署是自己做的吗？还是我们直接去消费了中间件那边

项⽬简介：

负责对接 sas 平台实现⽇志采集分析，整合相关安全信息，实现接⼝敏感信息⾃动检测、流量异常告警。 对接⽇志存储中心进⾏⽇志分析，建⽴安全部⻔的统⼀管理平台以应对⽇益增⻓的安全⻛险，提⾼突发事件处理追溯能⼒。

项⽬细节和成果： 
1. 业务⽅输出⽇志到kafka，sas平台通过消费 kafka 中的消息实现每⽇⽇志收集。通过 Django 后端实现 api 信息、⽹关数据、服务 信息、业务负责⼈信息的聚合，实现总数 7000+api 信息追踪。分析⽇志⽂件，将接⼝ body 等信息定时更新⼊库，已累计发现接 ⼝ 13000+，累计发现 100+公⽹暴露的淘汰接⼝。 
2. 接⼊企业微信机器⼈，实现突发安全事件实时告警，安全巡检信息定时通报。 
3. 使⽤ Vue 框架开发安全管理平台前端，实现数据图像化展示、api 信息前端增删改、指定字段聚合搜索、⼀键操作业务等功
4. 使⽤ Docker 实现⾃动构建部署。


日志的收集和处理，通过什么方式拿到日志，然后怎样进行处理？

1. 业务方在服务器上将日志写入到日志文件，我们通过Filebeat监控这个日志文件是否有更新，并把更新的内容作为消息发送到对应topic的kafka中。
2. 我们的日志管理平台里有kafka的消费者，当kafka中有新增消息时我们会对应的去消费这些消息，并根据不同的topic对日志进行分类。
3. 日志进入平台之后先提取日志中的有效字段（日志基本为json格式，如果不是json格式需要定制一些脚本进行提取），提取后将日志存入es。
4. 通过es的聚合查询查询日志来提取出每天的去重后的接口，与接口管理平台的redis接口集合进行对比，发现新增的接口并继续将这些接口添加到集合中。发现新的接口之后用脚本去调中间件那边的接口获取对应的信息然后存储在mysql数据库里。把接口对应的request、api、body这些东西存储起来

Filebeat实时日志监控？

1. Input组件负责监控日志文件(目录)自身的变化情况，如某文件被移动、删除了或创建了新文件，然后将这些信息提供给Harvester使用。
2. Harvester组件（收割机）用于监控一个日志文件内容的变化情况。如，是否新加了一行数据，是否读到了EOF等。
3. Spooler组件用于将Harvester"收割"到的新事件(日志行)发送到Kafka对应的topic中。
4. 所有服务器的日志文件映射到k8s根节点的服务器上，然后直接通过采集k8s根节点的日志来完成

FileBeat的部署方式？

大体上，在当前的K8s架构中采集日志通常有以下几种常见方式：
1. 在每个节点上部署日志采集Agent
2. 使用sidecar模式在业务Pod内部署日志采集容器
3. 在业务应用内直接向服务端发送日志

这里我们使用的是第一种方式：在每个节点上部署日志采集Agent

在这种方式下，日志采集Agent通常以一个能访问节点上所有日志的容器存在。通常生产集群有很多节点，每个节点都需要部署一个采集Agent。面对这种情况，最简单的部署方式是直接只用K8s提供的Deployment进行容器编排。DaemonSet controller会定期检查节点的变化情况，并自动保证每个节点上有且只有一个采集Agent容器。（具体怎么实现的不太清楚）

filebeat.yml 配置

1. paths（日志文件路径）、hosts（kafka主机ip和端口）、
topic（kafka主题）、name（本机IP）、logging.level（filebeat日志级别）
2. 以后台守护进程启动启动filebeats `nohup ./filebeat -e -c filebeat.yml &`
3. 难点就是filebeat.yml的配置踩了许多坑。

Elasticsearch

如何消费Kafka中新的消息？

1. 我们采用的是【发布订阅模式的消息队列】消费部署在多台机器上的生产者产生的消息
2. 写消费者的脚本，利用Kafka这个库中的KafkaConsumer来生成一个消费者群组，订阅topic后通过轮询去向服务器请求数据，拿到partiti返回的数据。轮询的细节不清楚，只知道会处理群组心跳、获取数据这些细节。
3. 根据不同的topic对日志进行分类之后提取其中的有效字段。日志的格式是json的，这是filebeat处理的

怎么样才可以知道性能到达了瓶颈？

消费达到瓶颈的话一般是看kafka的lag数量，这个数量代表kafka消息队列中堆积的消息的数量，如果达到瓶颈的话这个lag就会上升

消费日志的性能有瓶颈怎样去扩展？

消费有瓶颈的话一般是多加几个消费者，相当于给一个topic更多的partition，比如原来可能是8个consumer来消费这个topic，现在发现有瓶颈就给这个topic更多的分区，每个分区对应一个consumer

同时也要看是所有consumer都达到瓶颈还是个别几个consumer的lag特别高，个别几个consumer的lag特别高的话就可能要考虑是不是那一台消费者服务器上部署了太多的消费者，需要做一个平衡



python kafka脚本

1. 消费端

    from kafka import KafkaConsumer
    
    consumer = KafkaConsumer('my_topic', group_id= 'group2', bootstrap_servers= ['localhost:9092'])
    for msg in consumer:
        print(msg)

- 第1个参数为 topic的名称
- group_id : 指定此消费者实例属于的组名，可以不指定
- bootstrap_servers ： 指定kafka服务器

2. 消费者端接收消息如下：


    ConsumerRecord(topic='my_topic', partition=0, offset=4, timestamp=1529569531392, timestamp_type=0, key=b'my_value', value=None, checksum=None, serialized_key_size=8, serialized_value_size=-1)

- topic 
- partition 
- offset ： 这条消息的偏移量 
- timestamp ： 时间戳 
- timestamp_type ： 时间戳类型 key ： key值，字节类型 
- value ： value值，字节类型 checksum ： 消息的校验和 
- serialized_key_size ： 序列化key的大小 
- serialized_value_size ： 序列化value的大小，可以看到value=None时，大小为-1

3. 手动分配partition


    from kafka import KafkaConsumer
    from kafka import TopicPartition
    
    consumer = KafkaConsumer(group_id= 'group2', bootstrap_servers= ['localhost:9092'])
    consumer.assign([TopicPartition(topic= 'my_topic', partition= 0)])
    for msg in consumer:
        print(msg)

4. 超时处理


    consumer = KafkaConsumer('my_topic', group_id= 'group2', bootstrap_servers= ['localhost:9092'], consumer_timeout_ms=1000)

若不指定 consumer_timeout_ms，默认一直循环等待接收，若指定，则超时返回，不再等待

5. 订阅多个topic


    from kafka import KafkaConsumer
    
    consumer = KafkaConsumer(group_id= 'group2', bootstrap_servers= ['localhost:9092'])
    consumer.subscribe(topics= ['my_topic', 'topic_1'])
    for msg in consumer:
        print(msg)





# 面试问题

## 概念和原理

1. 列表和元组的区别
    1. 列表是可变对象，元组是不可变对象。 
   3. 合并元组生成新元组的时候也不会为其分配额外空间（c代码中的resize） 
   4. 时间复杂度是On的而不是列表的O1，这是因为每次都会生成新的对象 而列表在额外空间耗尽时才会这么做（不同的列表生成方式也有所不同）

2. while-else
    只有当while循环正常结束的情况下，才执行else块中的语句，当while 块遇到break强制跳出的时候，else 块中的语句不被执行。
    ```
    a = 2
    while a >= 0:
        print(a)
        a -= 1
        break
    else:
        print('OK')
    ```
    这个循环中因为有break直接跳出了，所以不会输出OK。如果没有break，在while循环因为`a = -1`判断语句为False正常结束的情况下会在控制台输出OK。
    
3. try-except-else-finally
    如果在执行 try 块里的业务逻辑代码时出现异常，系统自动生成一个异常对象，该异常对象被提交给 Python 解释器，这个过程被称为引发异常。当 Python 解释器收到异常对象时，会寻找能处理该异常对象的 except 块，如果找到合适的 except 块，则把该异常对象交给该 except 块处理，这个过程被称为捕获异常。如果 Python 解释器找不到捕获异常的 except 块，则运行时环境终止，Python 解释器也将退出。
    在使用一个 except 块捕获多种类型的异常时，只要将多个异常类用圆括号括起来，中间用逗号隔开即可，其实就是构建多个异常类的元组`except (IndexError, ValueError, ArithmeticError):`
    
    当 try 块没有出现异常时，程序会执行 else 块.
    
    如果 else 和 finally 都存在的话，else 必须在 finally 之前，finally 必须在整个程序的最后。finally代码块无论try中的语句是否发生异常都将执行。
    ```
    def fun():
        try:
            print('tyr')
            return 1
        except:
            print('except')
            return 'erro'
        else:
            print('else')
            return 'success'

    res = fun()
    print(res)
    # tyr
    # 1

    print('-------------------------------------------------------------')

    def fun():
        try:
            print('tyr')
            return 1/0
        except:
            print('except')
            return 'erro'
        else:
            print('else')
            return 'success'

    res = fun()
    print(res)
    # tyr
    # except
    # erro
    print('-------------------------------------------------------------')

    def fun():
        try:
            print('tyr')
            return 1/0
        except:
            print('except')
            return 'erro'
        else:
            print('else')
            return 'success'
        finally:
            print('finally')
            return 'f'

    res = fun()
    print(res)
    # tyr
    # except
    # finally
    # f
    print('-------------------------------------------------------------')

    def fun():
        try:
            print('tyr')
            return 1/0
        except:
            print('except')
            return 'erro'
        else:
            print('else')
            return 'success'
        finally:
            print('finally')

    res = fun()
    print(res)
    # tyr
    # except
    # finally
    # erro
    ```
    finally代码块是一定要执行的，当finally代码块存在时会在try、except、else代码块中遇到return或者运行结束的时候运行finally代码块中的语句。如果此finally代码块中有return语句，则会返回finally中的值。否则在执行完finally代码块后回到try、except、else代码块中执行return语句。
    

2. mutable和immutable

    可变对象和不可变对象
    
    通过一段代码是没有办法直接判断Object是否是可变对象，python本身也是不能判断的。实现这个数据结构的方式定义了这个类是否是可变的。
    
    不可变对象---内部状态不可变的对象
    
    python对可变和不可变对象的执行是一样的
    
        l = [1,2,3]
        t = (1,2,3)
    
        l[0] = 2
        t[0] = 2
    
    给tuple赋值的时候失败是因为tuple的setitem函数不允许这样赋值

    只要一个数据结构从c层面实现的时候把【改变内部状态】的路都封死，这个object就变成不可变对象了

5. 列表和元组执行效率
   1. 由于列表是动态的，所以它需要存储指针来指向对应的元素
   7. 另外，由于列表可变，所以需要额外存储已经分配的长度大小，并且python在实现上还会提前分配一部分内存给列表（我记得是double）
   8. python还会对静态数据做一些缓存，占用空间不大的时候这部分内存不会被立刻回收而是会缓存下来，再创建同样大小的元组时，Python 就可以不用再向操作系统发出请求去寻找内存，而是可以直接分配之前缓存的内存空间。
   9. 总体来说元组对于列表更加轻量级，性能稍优。

10. python的多线程机制
    1. 多线程本身是为了解决计算机处理多个任务时的协作和抢占的，慢慢操作系统也发展出了完善的任务调度来支持这个功能，程序员不用自己实现这一部分。
    12. 概念：
        1. 物理cpu数是主板上实际插入的硬件个数socket
        14. 核心是是物理cpu上的的运算核心
    15. 线程私有资源：
        1. 线程运行的本质其实就是函数的执行，函数的执行总会有一个源头，这个源头就是所谓的入口函数，CPU从入口函数开始执行从而形成一个执行流，只不过我们人为的给执行流起一个名字，这个名字就叫线程。
        17. 线程运行的本质就是函数运行，函数运行时信息是保存在栈帧中的，因此每个线程都有自己独立的、私有的栈区。
        18. 函数运行时需要额外的寄存器来保存一些信息，像部分局部变量之类，这些寄存器也是线程私有的，一个线程不可能访问到另一个线程的这类寄存器信息。
        19. 所属线程的栈区、程序计数器、栈指针以及函数运行使用的寄存器是线程私有的。 以上这些信息有一个统一的名字，就是线程上下文，thread context。
    20. 线程共享资源
        1. 代码区：编译后的可执行机器指令，是从从可执行文件中加载到内存的。这就意味着程序中的任何一个函数都可以放到线程中去执行，不存在某个函数只能被特定线程执行的情况。
        22. 数据区：进程地址空间中的数据区，这里存放的就是所谓的全局变量。在程序运行期间，也就是run time，数据区中的全局变量有且仅有一个实例，所有的线程都可以访问到该全局变量。
        23. 堆区：只要知道变量的地址，也就是指针，任何一个线程都可以访问指针指向的数据，因此堆区也是线程共享的属于进程的资源。
        24. 栈区：栈区属于线程私有这一规则并没有严格遵守。不像进程地址空间之间的严格隔离，线程的栈区没有严格的隔离机制来保护，因此如果一个线程能拿到来自另一个线程栈帧上的指针，那么该线程就可以改变另一个线程的栈区，也就是说这些线程可以任意修改本属于另一个线程栈区中的变量。
        26. 文件：如果程序在运行过程中打开了一些文件，那么进程地址空间中还保存有打开的文件信息，进程打开的文件也可以被所有的线程使用，这也属于线程间的共享资源。
    27. 全局解释锁GIL(Global Interpreter Lock)
        1. 要支持多线程的话，一个基本的要求就是不同线程对共享资源访问的互斥，所以Python中引入了GIL
        29. Python中的GIL是一个非常霸道的互斥实现，在一个线程拥有了解释器的访问权之后，其它的所有线程都必须等待它释放解释器的访问权，即使这些线程的下一条指令并不会互相影响。
        30. 对于多处理器，同一时间，确实可以有多个线程独立运行在核心上。然而正是由于GIL限制了这样的情形，使得多处理器最终退化为单处理器
        31. 对于Python而言，字节码解释器是Python的核心所在，所以Python通过GIL来互斥不同线程对解释器的使用。
    32. 问题：现在有三个线程A、B和C，它们都需要解释器来执行字节码，进行对应的计算，那么在这之前，它们必须获得GIL。那么现在假设线程A获得了GIL，其它线程只能等A释放GIL之后，才能获得
        1. 线程A何时释放GIL呢（如果A使用完解释器之后才释放GIL，那么，并行的计算退化为串行，多线程的意义何在？）
        34. 线程B和C谁将在A释放GIL之后获得GIL呢？
    35. 线程调度
        1. Python中通过软件模拟了操作系统中对于进程切换的时钟中断和对时钟中断的响应。
        37. Python的字节码解释器是按照指令的顺序一条一条的顺序执行从而工作的，Python内部维护着这样一个数值，作为Python内部的时钟，假设这个值为N，那么Python将在执行了N条指令之后立刻启动线程调度机制。
        38. 也就是说，当一个线程获得GIL后，Python内部的监测机制就开始启动，当这个线程执行了N条指令后，Python解释器将强制挂起当前线程，开始切换到下一个处于等待状态的线程。
        39. 下一个执行的线程对python来说是不可知的，因为这是底层的操作系统来实现的。Python借用了底层操作系统所提供的线程调度机制来决定下一个获得GIL进入解释器的线程是谁。 所以说，Python中的线程实际上就是操作系统所支持的原生线程。
    40. GIL的获得
        1. 在Python虚拟机启动时，多线程机制并没有被激活，它只支持单线程，一旦用户调用thread.start_new_thread，明确的告诉Python虚拟机需要创建新的线程，这时Python意识到用户需要多线程的支持，这个时候，Python虚拟机会自动建立多线程需要的数据结构、环境以及GIL。建立多线程环境，主要就是创建GIL
        42. 获得的GIL是一个c的结构体，其中记录了三个变量：
            1. hevent：内核对象，也就是通过内核对象来实现线程之间的互斥。
            44. thread_id：将记录任一时刻获得GIL的线程的id
            45. owned：指示GIL是否可用的变量
46. 和其他语言的多线程区别？
    1. Python中的多线程是单CPU意义上的多线程
48. Python的内存管理的层级
    1. block，最小的层级
       1. 每个block保存一个固定大小的数据
    2. pool，4k和一个内存页一样大，大小是相同的
       1. 每个pool中的block大小都是相同的
       2. 不同pool中的block大小可能是不同的
    3. arena，最大的层级，256k。
       1. 记录arena中未使用的pool的数量
       2. 每个pool中的block大小是在这个pool被使用的时候决定的
    4. 这样大小比较小的对象（小于等于512byte，否则从c库用malloc）在使用内存的时候就不会去和系统频繁申请内存。
    5. python向系统free内存的维度是从arena的层级实现的，当arena中还有内存在被使用的时候就不会被free。
    6. 只有所有的arena中的pool都被用满了之后才会去申请新的arena，同时python会对arena剩余内存的大小进行排序，优先从剩余小的arena中申请内存
    3. 垃圾回收机制 https://zhuanlan.zhihu.com/p/295062531
       1. 引用计数：
          1. Python 中有个内部跟踪变量叫做引用计数器，对每个对象维护一个引用计数，当对象的引用计数为 0 时，就列入了垃圾回收队列。
          2. Python 中的每个变量都是对真实对象的引用（指针），为了跟踪引用，每个对象（包括整数、字符串）都会有一个引用计数的额外字段，该字段会一直维护，在创建或删除指向该对象的引用时会增加或减少。
          2. 引用计数增加的情况：
            * 赋值运算符
            * 参数传递
            * 将对象（作为元素）加入到容器中
          3. 引用计数减少的情况：
            * 对象的引用变量被显示销毁
            * 对象的引用变量赋值引用其他对象
            * 对象从容器中被移除，或者容器被销毁
            * 一个引用离开了它的作用域
            * 
       2. 分代收集
          1. 一种优化策略，目前很流行。算法按照对象生命周期长短划分不同的代空间，生命周期长的放入老年代，短的放入新生代，从而区别不同的回收算法策略和频率，辅助使用
          2. 它只处理循环引用
1. 闭包
   1. 有时候需要在函数外部得到函数内的局部变量。但是，由于Python中作用域的搜索顺序（"链式作用域"结构（chain scope）：子对象会一级一级地向上寻找所有父对象的变量），这一点通常是无法实现的。
    ```python
    def f1():
        n = 999
    print(n)
   # NameError
    ```
   2. 但是有一种方法除外，那就是在函数的内部，再定义一个函数。f2可以读取f1中的局部变量，那么只要把f2作为返回值, 就可以在f1外部读取它的内部变量
   ```python
    def f1():
        n=999
        def f2():
            print(n)
    
        return f2

    result = f1()
    result()
    ```
   3. 上一部分代码中的f2函数，就是闭包。
   4. 定义： 闭包就是能够读取外部函数内的变量的函数
   5. 作用：
      1. 闭包是将外层函数内的局部变量和外层函数的外部连接起来的一座桥梁。
      2. 将外层函数的变量持久地保存在内存中。
      ```python
      def create(pos=None):
          if pos is None:
              pos = [0,0]
      
          def go(direction, step):
              new_x = pos[0]+direction[0]*step
              new_y = pos[1]+direction[1]*step
      
              pos[0] = new_x
              pos[1] = new_y
          return go
      
      player = create()
      
      print(player([1,0],10))
      print(player([0,1],20))
      print(player([-1,0],10))
      # [10, 0]
      # [10, 20]
      # [0, 20]
      ```
      原因就在于create是go的父函数，而go被赋给了一个全局变量，这导致go始终在内存中，而go的存在依赖于create，因此create也始终在内存中，不会在调用结束后，被垃圾回收机制（garbage collection）回收。
   6. 闭包无法改变外部函数局部变量指向的内存地址，参考命名空间和作用域。
   7. 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。
      
2. frame 栈帧
    
    所有python代码都会在编译期先被编译成code object，code object本身是不会保存运行时的信息的，这些信息都交给frame去保存了，比如说记录一个局部变量。每一次这个code object被调用都会新建一个frame。frame保存在一个栈中。
        
        sys._getframe()

    
        import inspect
 
def f():
frame = inspect.currentframe()
print(frame.__dir__())
f()

    l = ['f_back', 'f_code', 'f_builtins', 'f_globals', 'f_lasti', 'f_locals', 'f_lineno', 'f_trace']

## 语言特性
1.  Python字符串反转的几种操作？
   2. s = ''.join(reversed(s))
   3. s[::-1]
2. 匿名函数的好处？
   1. 减少重复代码
   2. 不用起名
   3. 减少其他位置的错误调用

## MySQl
### 基本语句和用法
使用MySQL

    SHOW DATABASES;
    USE database;
    SHOW TABLES;
    SHOW columns FROM table;
    DESCRIBE columns;
    SHOW STATUS;
    SHOW GRANTS

检索数据

    SELECT column1, column2 FROM table;
    SELECT * FROM table;
    SELECT DISTINCT column1, column2 FROM table;
    注意此时是将1、2列所有不同的组合全部列出来，作为一个组合去去重，组合重复是指是1、2元素都相等
    SELECT * FROM table LIMIT 5;
    返回不多于5行
    SELECT * FROM table LIMIT 5,2;
    从行号为5的位置开始，返回6、7行
    SELECT column1, column2 FROM table ORDER BY column1;
    SELECT column1, column2 FROM table ORDER BY column1, colunm2;
    按多个列排序时，排序完全按所规定的顺序进行。
    SELECT column1, column2 FROM table ORDER BY column1 DESC;
    SELECT column1, column2 FROM table ORDER BY column1 DESC, colunm2;
    只对column1降序排列，DESC关键字只应用到直接位于其前面的列名

过滤数据

    SELECT column1, column2 FROM table WHERE column1 = 1;
    在同时使用ORDER BY和WHERE子句时，应 该让ORDER BY位于WHERE之后，否则将会产生错误
    =; <>; !=; <; >; <=; >=; BETWEEN a AND b ;
    SELECT column1, column2 FROM table WHERE column1 is NULL;
    
    AND, OR, IN, NOT
    SELECT column1, column2 FROM table WHERE column1 = 1 AND column1 = 2;
    SELECT column1, column2 FROM table WHERE column1 = 1 AND column2 = 2;
    SQL（像多数语言一样）在处理OR操作符前，优先处理AND操作符, 此问题的解决方法是使用圆括号明确地分组相应的操作符
    SELECT column1, column2 FROM table WHERE (column1 = 1 OR column1 = 2) AND column2 = 2;
    SELECT column1, column2 FROM table WHERE column1 IN (1,2);  等价于
    SELECT column1, column2 FROM table WHERE column1 = 1 OR column1 = 2;
    SELECT column1, column2 FROM table WHERE column1 NOT IN (1,2);

通配符

    SELECT column1, column2 FROM table WHERE column1 LIKE 'jet%';
    找到用jet开头的行，大小写不区分，%匹配之后的任意字符，类似 .*
    SELECT column1, column2 FROM table WHERE column1 LIKE '%jet%';
    找到包含jet的行
    SELECT column1, column2 FROM table WHERE column1 LIKE 'jet_';
    _匹配一个字符，类似 . 
    SELECT column1, column2 FROM table WHERE column1 REGEXP '\\.';
    表示查找 .

创建计算字段
    
    Concat, AS
    SELECT Concat(column1, '(', column2, ')') FROM table;
    SELECT Concat(column1, '(', column2, ')') AS concat FROM table;

    +-*/
    SELECT column1, column2*column3 AS result, column3 FROM table;

汇总数据

    AVG()       返回某列的平均值
    COUNT()     返回某列的行数
    MAX()       返回某列的最大值
    MIN()       返回某列的最小值
    SUM()       返回某列值之和
    SELECT AVG(column1) AS avg FROM table AS concat;
    AVG()函数忽略列值为NULL的行
    SELECT SUM(column2*column3) AS result FROM table;
    聚集函数都可用来执行多个列上的计算

分组数据

    SELECT column1, COUNT(column2) AS count FROM table GROUP BY column1;
    GROUP BY子句指 示MySQL按column1排序并分组数据。这导致对每个column1而不是整个表计算count一次
    SELECT column1, COUNT(column2) AS count FROM table GROUP BY column1 HAVING count > 2;
    在分组后过滤count > 2的组

子查询

    SELECT column1 
    FROM table1
    WHERE column2 IN (SELECT column2
                      FROM table2
                      WHERE column3 = 1);

联结

    外键（foreign key）为某个表中的一列，它包含另一个表的主键值，定义了两个表之间的关系。

插入数据

    INSERT INTO table1
    VALUES(NULL, 'text');
    第一列cust_id也为NULL。这是因为每次插入一个新行时，该列由MySQL自动增量。你不想给出一个值（这是MySQL的工作），又不能省略此列（如前所述，必须给出每个列），所以指定一个NULL值（它被MySQL忽略，MySQL在这里插入下一个可用的cust_id值）。

更新数据

    UPDATE tbale SET column1 = 1 WHERE column2 = 2;
    UPDATE tbale SET column1 = 1, column3 = 3 WHERE column2 = 2;

删除数据
    
    DELETE FROM tbale  WHERE column1 = 1;

数据库大表优化方案

    不知道

### Mysql的存储引擎分类
   * InnoDB: 支持事务，行锁及无锁读提高了并发的效率，为了数据的完整性支持外键
   * MyISAM: 不支持事务和外键，表级别锁，优势在于访问速度快，一般用于只读或者以读为主的数据场景。

### 事务

事务只是一些改变，是一些操作的集合，是一个程序的执行单元，本身并不包含ACID的特性。当我们用各种手段让这个执行单元满足ACID的特性后我们就可以称这个执行单元为一个“正确的事务”。事务中的这些操作要么一起成功，要么一起失败。

#### ACID特性

* 原子性(Atomicity)：表示事务是一个不可分割的工作单位,事务中的操作要么全部成功、要么全部失败，通过undolog实现

* 一致性(Consistency)：表示事务操作前后的数据完整性要保持一致。确保一致性是用户的责任，而不是数据库的责任。A要向B支付100元,而A的账户中只有90元

  1. 我们给定账户余额这一列的约束是，不能小于0。那么很明显这条事务执行会失败
  2. 业务上不允许账户余额小于0。因此支付完成后我们会检查A的账户余额，发现余额小于0了，于是我们进行了事务的回滚。
  3. 一致性应该是应用系统从一个正确的状态到另一个正确的状态，并不能够由数据库管理系统保证。

* 隔离性(Isolation)：并发情况下操作数据库时,多个事务之间是相互隔离的,操作是互相不影响的,可以通过设置隔离级别来控制,具体下面会举例说明。如: A事务修改了C记录的年龄,B事务同时读取C记录的年龄,这时候B事务读取到的年龄还是A事务修改之前的，因为A事务没有提交(这个案例是以:Mysql数据库进行举例,默认的隔离界别是可重复读)。可以通过加锁（影响性能）或者MVCC解决。

* 持久性(Durability)：事务对数据库的数据影响是永久的,只要提交了事务,那么数据库的数据就被更改了,即使数据库出现故障,也不会对这个数据造成影响。通过redolog实现，数据读取到内存中发生修改后在刷新对应的磁盘页的之前先在redolog中写一遍，磁盘寻址是随机IO，redolog写日志是顺序IO。

### 日志系统

MySQL主要的日志系统是redolog、binlog、undolog。

#### 数据更新流程：

1. 获取数据，数据不在内存中则从磁盘读取
2. 返回数据
3. 更改数据
4. 写入新的数据
5. 数据更新到内存中
6. 写入redolog，处于prepare状态
7. 写binlog
8. 提交事务，处于commit状态

如果在操作A数据库先写redolog而没有写完binlog的时候服务挂了，B数据库是通过binlog来进行数据同步的，则会出现数据不一致，主从同步失败。

如果在操作A数据库先写binlog而没有写完redolog的时候断电恢复数据，B数据库是通过binlog来进行数据同步的，那么A、B数据库的数据会出现不一致。

### MVCC机制（多版本并发控制）

通俗的讲就是MVCC通过对数据进行多版本保存，根据比较版本号来控制数据是否展示，从而达到读取数据时无需加锁就可以实现事务的隔离性。

#### 并发情况

* 读读：不存在任何问题，不需要并发控制

* 读写：脏读、幻读、不可重复读

* 写写：可能存在更新丢失

#### 读取数据的方式：

* 当前读
    读取数据的时候读取的都是最新版本的数据
* 快照读
    读取的是历史版本的数据（最普通的SELECT操作）

#### 隐藏字段

DB_TRX_ID：创建这条记录或者最后一次修改该记录的事务id（在事务操作的时候事务id是递增的）
DB_ROLL_PTR：回滚指针，指向数据的上一个版本
DB_ROW_ID：隐藏主键，如果没有显式主键的话，就会多一个隐藏主键

#### undolog

undolog中会形成一个链表，链首是最新的旧记录，链尾是最旧的旧记录。purge机制会清楚数据防止undolog无限膨胀。

#### readview

事务进行快照读操作的时候产生的读视图，包含一下字段：
* trx_list：readview生成时刻当前系统活跃的事务id
* up_limit_id：活跃列表中最小的事务id
* low_limit_id：系统尚未分配的下一个事务id
然后生成的readview会根据可见性算法来判断是否可以读取到对应的数据结果。

在RC隔离级别中，每次进行快照读操作的时候都会重新生成新的readview，所以每次可以炒寻到最新的结果记录。

在RR隔离级别中，只有当前事务在第一次进行快照读的时候才会生成readview，之后的快照读操作都是用的第一次的readview。

#### MVCC机制实现原理：
* undo log：innodb在修改数据库数据记录之前会先在undo log中记录回滚日志，日志的内容为：执行insert时会对应在undo log中记录一条delete语句，并且会记录这个版本的事务id(txid)，执行update语句会有一条update语句来使之数据恢复到上个版本。
* 一致性视图：InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。简而言之一致性视图是个数组，记录了当前活跃的事务ID，通过这个数据我们可以判断出来事务执行的先后顺序，事务所能读取的数据版本。在innodb 中每个SQL语句执行前都会得到一个read_view。副本主要保存了当前数据库系统中正处于活跃（没有commit）的事务的ID号，其实简单的说这个副本中保存的是系统中当前不应该被本事务看到的其他事务id列表。数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

### 解决并发问题的通用方案有：
* 对并发访问的数据添加一把排它锁，添加锁之后，其他的读和写操作都需等待锁释放后才能访问。
* 添加一把共享锁，读读操作不需要等待锁的释放，读写和写写操作需要等待锁的释放。
* 通过对并发数据进行快照备份，从而达到无锁数据的并发访问。

#### 事务的隔离级别

2. 读未提交(Read Uncommitted): 会导致脏读、不可重读读、幻读问题。
2. 读已提交(Read Committed): 会导致不可重复读、幻读问题(PostgreSQL数据库默认的隔离级别)。
3. 可重复度(Repeatable Read): 会导致幻读问题(Mysql数据库默认的数据库隔离级别)。
4. 串行化(Serialized):不存在任何问题,但是这个效率非常低,同一时刻只能有一个用户可以操作数据库,只有这个用户操作完了,其他的用户才能进行操作。

2. 可能存在的问题 https://bbs.huaweicloud.com/blogs/342963
    1. 更新丢失: 并发事务时，可能出现多个事务同时更新同一条记录，导致前一个事务更新的被后面事务的更新覆盖。
   2. 脏读: 一个事务读取到另一个事务没有提交的数据
   3. 不可重复读: 在同一个事务中,前后读取的相同的条件下的数据不一样(在并发情况下另外一个事务对数据进行了修改)，数据单行中的内容发生变化
   4. 幻读: 同一个事务下,前后读取的数据不一样(在并发情况下,另外的事务对数据进行了删除或者增加的操作)，数据行数发生变化。幻读的本质是在RR隔离级别下快照读和当前读一起使用产生读取不一致的问题。


### SQL 分类：

SQL 语句主要可以划分为以下 3 个类别。

* DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。

* DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查）

* DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。

### 可能产生死锁的原因和解决

死锁产生的原因：
1. 互斥使用
2. 占有且等待
3. 不可抢占
4. 循环等待

解决死锁：
1. 互斥使用一般不破坏
2. 占有且等待：一次性请求所有资源，不能成功则等待
3. 不可抢占：尝试获取锁时超时就释放占有的所有资源
4. 循环等待：银行家算法，不懂。

### 锁的粒度

#### 表锁

表级锁是 MySQL 锁中粒度最大的一种锁，表示当前的操作对整张表加锁，资源开销比行锁少，不会出现死锁的情况，但是发生锁冲突的概率很大。被大部分的mysql引擎支持，MyISAM和InnoDB都支持表级锁，但是InnoDB默认的是行级锁。

表锁由 MySQL Server（MySQL 服务器层）实现，一般在执行 DDL 语句时会对整个表进行加锁，比如说 ALTER TABLE 等操作。在执行 SQL 语句时，也可以明确指定对某个表进行加锁。

表锁使用的是一次性锁技术，也就是说，在会话开始的地方使用 lock 命令将后续需要用到的表都加上锁，在表释放前，只能访问这些加锁的表，不能访问其他表，直到最后通过 unlock tables 释放所有表锁。

除了使用 unlock tables 显示释放锁之外，会话持有其他表锁时执行lock table 语句会释放会话之前持有的锁;会话持有其他表锁时执行 start transaction 或者 begin 开启事务时，也会释放之前持有的锁。

#### 行锁

行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。有可能会出现死锁的情况。 行级锁按照使用方式分为共享锁和排他锁。

不同存储引擎的行锁实现不同，后续没有特别说明，则行锁特指 InnoDB 实现的行锁。

在了解 InnoDB 的加锁原理前，需要对其存储结构有一定的了解。InnoDB 是聚簇索引，也就是 B+树的叶节点既存储了主键索引也存储了数据行。而 InnoDB 的二级索引的叶节点存储的则是主键值，所以通过二级索引查询数据时，还需要拿对应的主键去聚簇索引中再次进行查询。关于MySQL索引的详细知识可以查看《MySQL索引底层数据结构与算法》。

#### 记录锁

行锁的一种，但是范围是表中的某一条记录，加了记录锁之后可以避免数据在查询的时候被修改的不可重复读问题，业务避免了在修改的事务未提交前被其他事务读取的脏读问题。这是内存和磁盘交互的时候读取数据页以系统规定的大小读取。

列必须是必须为唯一索引列或主键列，否则上述语句加的锁就会变成临键锁。

#### 间隙锁

属于行锁的一种，间隙锁是在事务加锁后锁住的是表记录的某一个区间，当表的相邻id之间出现空隙则会形成一个区间，遵循左开右闭原则。只会出现在可重复读的事务级别中。

唯一索引只有锁住多条记录或者一条不存在的记录的时候，才会产生间隙锁，指定给某条存在的记录加锁的时候，只会加记录锁，不会产生间隙锁。

普通索引不管是锁住单条，还是多条记录，都会产生间隙锁；间隙锁会封锁该条记录相邻两个键之间的空白区域，防止其它事务在这个区域内插入、修改、删除数据，这是为了防止出现 幻读 现象；

和行锁有冲突的关系的是“另外一个行锁”，但是跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。

举例来说，假如emp表中只有101条记录，其empid的值分别是1,2,...,100,101，下面的SQL：
```mysql
SELECT * FROM emp WHERE empid > 100 FOR UPDATE
```
当我们用条件检索数据，并请求共享或排他锁时，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。

间隙锁造成的死锁
间隙锁的引入会使同样的语句锁住更大的范围，这其实是影响并发度的，有时候还会更容易造成死锁，看下面场景：

session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);
session B 执行 select … for update 语句，由于 id=9 这一行并不存在，因此也会加上间隙锁 (5,10);
session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待
session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了

#### 临键锁

```mysql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;

insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```

行锁 + 该行之前的间隙锁 合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]（supremum只是为了保证开闭区间的一致性，可以理解为无穷大）。

next-key lock的加锁过程是先加间隙锁再加行锁，这不是一个原子性操作，因此会出现只加了间隙锁但加行锁被阻塞的情况，比如下面这个情况：

1. session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock (5,10] 和间隙锁 (10,15)；
2. session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，但是只有间隙锁(5, 10) 加成功了，行锁[10]进入锁等待；
3. 然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。

#### 页锁

页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁。

### 锁的使用方式

#### 共享锁(Share Lock)

共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改(获取数据上的排他锁)，直到已释放所有共享锁。

#### 排他锁(eXclusive Lock)

排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。

如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。

### 事务逻辑上的锁

#### 悲观锁

在关系数据库管理系统里，悲观并发控制(又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”)可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作对某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。

悲观锁，正如其名，它指的是对数据被外界(包括本系统当前的其他事务，以及来自外部系统的事务处理)修改持保守态度(悲观)，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制 (也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据)

1. 在对任意记录进行修改前，先尝试为该记录加上排他锁(exclusive locking);
2. 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定;
3. 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
4. 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。

悲观锁的优点和不足

悲观锁实际上是采取了“先取锁在访问”的策略，为数据的处理安全提供了保证，但是在效率方面，由于额外的加锁机制产生了额外的开销，并且增加了死锁的机会。并且降低了并发性;当一个事物锁住一行数据的时候，其他事物必须等待该事务提交之后，才能操作这行数据。

#### 乐观锁

在关系数据库管理系统里，乐观并发控制假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。

乐观锁( Optimistic Locking ) 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。

相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。

数据版本,为数据增加的一个版本标识。当读取数据时，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。

乐观锁的优点和不足

乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。

### 间隙锁(Gap Lock)

当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁;对于键值在条件范围内但并不存在的记录，InnoDB 也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。

间隙锁是锁索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。

间隙锁在 InnoDB 的唯一作用就是防止其它事务的插入操作，以此来达到防止幻读的发生，所以间隙锁不分什么共享锁与排他锁。


### 存储引擎和存储结构

一个索引是存储的表中一个特定列的值数据结构。索引是在表的列上创建。要记住的关键点是索引包含一个表中列的值，并且这些值存储在一个数据结构中。请牢记这一点：索引是一种数据结构。

存储引擎是基于文件系统的，抽象的数据需要转为文件来存储，基于数据的索引也需要持久化保存为文件。所以存储引擎需要存储“数据文件、索引文件”，数据文件的组织形式就是抽象的数据转化为文件的方式。

索引文件的组织形式决定了数据库的性能，所以我们也把索引文件的组织形式称为存储引擎的存储结构

索引组织表：数据记录存储在索引文件内部

#### 聚簇索引

innodb存储引擎中数据在进行插入的时候必须和某一个索引绑定在一起。如果表中有主键，那么跟主键绑定在一起，如果表中没有主键则和唯一键绑定，没有唯一键则和一个6字节的rowid绑定。

数据和索引放在一起的叫做聚簇索引，分开存储的叫做非聚簇索引。innodb两种都支持。

#### 二级索引（辅助索引）

一个表可以由N个索引，每个索引都是独立的一颗B+树，表中的数据存储1份，其他的非聚簇索引的叶子节点中存储的是聚簇索引中的Key值。

innodb中的非聚簇索引都可以称为二级索引或者辅助索引

#### 联合索引（复合索引）

类似于联合主键，一般情况下我们在设置索引列的时候只会选择一个列作为索引字段，但是某些情况下需要将多个列共同组成一个索引字段，称之为联合索引



### 存储结构的分类

in-place update structure：就地更新结构，如B树（Balanced tree）、B+树，直接覆盖旧记录来存储更新内容。读性能优，但是写会产生随机IO，性能差

out-of-place update structure：异位更新结构，如LSM树（log-structured merge tree）会将更新的内容存储到新的位置而不是覆盖旧的条目。顺序写，写性能高，需要数据整合过程避免数据无限制膨胀。

#### 存储结构的共性特点

对于磁盘存储介质来说：
1. 粒度大：IO需要尽量少，读取尽量连续。
2. 粒度小：允许并发操作，增删改查的影响小。这里的并发是物理磁盘上的概念，不是事务的并发。

| 分类   | lock       | latchs          |
|------|------------|-----------------|
| 隔离级  | 用户事务，逻辑上的锁 | 线程，物理上的锁，对用户不可见 |
| 保护对象 | 库中的数据      | 内存中的数据结构        |
| 持续时间 | 整个事务周期     | 临界区代码前后         |
| 类型   | 共享、互斥等     | 读、写等            |
| 持续时间 | 长          | 短               |


#### B树

以页为单位组织，innoDB中的一页大小为16k，由于B树有SMO操作（structure modification operation）操作，会造成页的分裂和合并，这种影响会向上传递。需要对可能有影响的结点加锁。

#### B+树

B+树构建规则：
1. B+树的非叶子节点不保存具体的数据，而只保存关键字的索引，而所有的数据最终都会保存到叶子节点。因为所有数据必须要到叶子节点才能获取到，所以每次数据查询的次数都一样，这样一来B+树的查询速度也就会比较稳定，而B树的查找过程中，不同的关键字查找的次数很有可能都是不同的（有的数据可能在根节点，有的数据可能在最下层的叶节点），所以在数据库的应用层面，B+树就显得更合适。

2. B+树叶子节点的关键字从小到大有序排列，B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。

3. 非叶子节点的子节点数=关键字数



## Redis

### 基本概念

#### Redis的特点
1. Redis本质上是一个非关系型的Key-Value类型的内存数据库，整个数据库统统加载在内存当中进行操作，所以读写速度非常快
2. 有丰富的数据结构，string字符串、哈希表、列表、集合、有序集合；提供交集、并集、差集等操作。
3. 单进程单线程，这里的单线程是指网络IO和键值对读写是通过串行队列由一个线程完成的（6.0以后引入的多线程是只是网络请求的多线程，所以redis仍然是并发安全的），其它的持久化和集群同步是由额外的线程执行的，具体并不了解，没有遇到过。
4. 

#### Redis中的数据结构

string 字符串
1. string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。
2. 一个键最大能存储512MB

Hash 哈希
1. Redis hash 是一个键值(key=>value)对集合，就像一个对象一样


    HMSET runoob field1 "Hello" field2 "World"
    HGET runoob field1
    HGETALL runoob
    HDEL key field1 [field2]
    删除一个或多个哈希表字段

List 列表

1. Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）
2. 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。

    
    redis 127.0.0.1:6379> LPUSH runoobkey redis
    (integer) 1
    redis 127.0.0.1:6379> LPUSH runoobkey mongodb
    (integer) 2
    redis 127.0.0.1:6379> LPUSH runoobkey mysql
    (integer) 3
    redis 127.0.0.1:6379> LRANGE runoobkey 0 10

    1) "mysql"
    2) "mongodb"
    3) "redis"

    BLPOP key1 [key2 ] timeout
    移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。
    BRPOP key1 [key2 ] timeout
    移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。

Set 集合
1. Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 
2. 集合对象的编码可以是 intset 或者 hashtable。
3. Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。


    redis 127.0.0.1:6379> SADD runoobkey redis
    (integer) 1
    redis 127.0.0.1:6379> SADD runoobkey mongodb
    (integer) 1
    redis 127.0.0.1:6379> SADD runoobkey mysql
    (integer) 1
    redis 127.0.0.1:6379> SADD runoobkey mysql
    (integer) 0
    redis 127.0.0.1:6379> SMEMBERS runoobkey

    1) "mysql"
    2) "mongodb"
    3) "redis"


#### Redis单线程为什么还能这么快？

1. 数据保存在内存中，内存操作非常快
2. 单线程命令操作没有线程进程切换开销
3. IO多路复用机制（不了解）
4. 数据结构有优化，跳表、list的双向链表、压缩列表

#### Redis底层存储原理

底层是一个全局哈希表

    SET zegging 'zegging'

先对key:zegging做一次哈希运算得到散列值，然后%全局哈希表的数组长度，存进对应index的链表中

### 基本语法
    
    启动服务端：redis-server
    启动客户端：redis-cli
    查看Redis版本号和连接数操作系统等信息：info

## Kafka

### 什么是 Kafka
Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统。

### Kafka 的基本术语

消息：Kafka 中的数据单元被称为消息，也被称为记录，可以把它看作数据库表中某一行的记录。

批次：为了提高效率， 消息会分批次写入 Kafka，批次就代指的是一组消息。

主题：消息的种类称为 主题（Topic）,可以说一个主题代表了一类消息。相当于是对消息进行分类。主题就像是数据库中的表。

分区：主题可以被分为若干个分区（partition），同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 kafka 的伸缩性，单一主题中的分区有序，但是无法保证主题中所有的分区有序

生产者： 向主题发布消息的客户端应用程序称为生产者（Producer），生产者用于持续不断的向某个主题发送消息。

消费者：订阅主题消息的客户端程序称为消费者（Consumer），消费者用于处理生产者产生的消息。

消费者群组：生产者与消费者的关系就如同餐厅中的厨师和顾客之间的关系一样，一个厨师对应多个顾客，也就是一个生产者对应多个消费者，消费者群组（Consumer Group）指的就是由一个或多个消费者组成的群体。

### 一、Events, Streams, Topics
在深入 Partition 之前，我们先看几个更高层次的概念，以及它们与 Partition 的联系。

1. Event（事件）代表过去发生的一个事实。简单理解就是一条消息、一条记录。 Event 是不可变的，但是很活跃，经常从一个地方流向另一个地方。

2. Stream 事件流表示运动中的相关事件。 当一个事件流进入 Kafka 之后，它就成为了一个 Topic 主题。

3. Topic 就是具体的事件流，也可以理解为一个 Topic 就是一个静止的 Stream。 Topic 把相关的 Event 组织在一起，并且保存。一个 Topic 就像数据库中的一张表。

### 二、Partition 分区

Kafka 中 Topic 被分成多个 Partition 分区。 Topic 是一个逻辑概念，Partition 是最小的存储单元，掌握着一个 Topic 的部分数据。 每个 Partition 都是一个单独的 log 文件，每条记录都以追加的形式写入。

Kafka分区下数据使用消息日志（Log）方式保存数据，具体方式是在磁盘上创建只能追加写（Append-only）消息的物理文件。因为只能追加写入，因此避免了缓慢的随机I/O操作，改为性能较好的顺序I/O写操作。Kafka日志文件分为多个日志段（Log Segment），消息被追加写到当前最新的日志段中，当写满一个日志段后Kafka会自动切分出一个新的日志段，并将旧的日志段封存。

### 三、Offsets（偏移量）和消息的顺序

Partition 中的每条记录都会被分配一个唯一的序号，称为 Offset（偏移量）。

Offset 是一个递增的、不可变的数字，由 Kafka 自动维护。

当一条记录写入 Partition 的时候，它就被追加到 log 文件的末尾，并被分配一个序号，作为 Offset。 向 Topic 发送消息的时候，实际上是被写入某一个 Partition，并赋予 Offset。

消息的顺序性需要注意，一个 Topic 如果有多个 Partition 的话，那么从 Topic 这个层面来看，消息是无序的。但单独看 Partition 的话，Partition 内部消息是有序的。所以，一个 Partition 内部消息有序，一个 Topic 跨 Partition 是无序的。 如果强制要求 Topic 整体有序，就只能让 Topic 只有一个 Partition。

### 四、Partition 为 Kafka 提供了扩展能力

一个 Kafka 集群由多个 Broker（就是 Server） 构成，每个 Broker 中含有集群的部分数据。

Kafka 把 Topic 的多个 Partition 分布在多个 Broker 中。

- 如果把 Topic 的所有 Partition 都放在一个 Broker 上，那么这个 Topic 的可扩展性就大大降低了，会受限于这个 Broker 的 IO 能力。把 Partition 分散开之后，Topic 就可以水平扩展 。
- 一个 Topic 可以被多个 Consumer 并行消费。如果 Topic 的所有 Partition 都在一个 Broker，那么支持的 Consumer 数量就有限，而分散之后，可以支持更多的 Consumer。
- 一个 Consumer 可以有多个实例，Partition 分布在多个 Broker 的话，Consumer 的多个实例就可以连接不同的 Broker，大大提升了消息处理能力。可以让一个 Consumer 实例负责一个 Partition，这样消息处理既清晰又高效。


### 五、Partition 为 Kafka 提供了数据冗余
Kafka 为一个 Partition 生成多个副本，并且把它们分散在不同的 Broker。

如果一个 Broker 故障了，Consumer 可以在其他 Broker 上找到 Partition 的副本，继续获取消息。

### 六、写入 Partition

#### 1. 使用 Partition Key 写入特定 Partition

Producer 发送消息的时候，可以指定一个 Partition Key，就可以写入特定 Partition 了。 Partition Key 可以使用任意值，例如设备ID、User ID。 Partition Key 会传递给一个 Hash 函数，由计算结果决定写入哪个 Partition。 所以，有相同 Partition Key 的消息，会被放到相同的 Partition。

例如使用 User ID 作为 Partition Key，那么此 ID 的消息就都在同一个 Partition，这样可以保证此类消息的有序性。

这种方式需要注意 Partition 热点问题。

#### 2. 由 kafka 决定
如果没有使用 Partition Key，Kafka 就会使用轮询的方式来决定写入哪个 Partition。

这样，消息会均衡的写入各个 Partition。

但这样无法确保消息的有序性。

#### 3. 自定义规则
Kafka 支持自定义规则，一个 Producer 可以使用自己的分区指定规则。


### 七、读取 Partition
Kafka 不像普通消息队列具有发布/订阅功能，Kafka 不会向 Consumer 推送消息。 Consumer 必须自己从 Topic 的 Partition 拉取消息。 一个 Consumer 连接到一个 Broker 的 Partition，从中依次读取消息。

消息的 Offset 就是 Consumer 的游标，根据 Offset 来记录消息的消费情况。 读完一条消息之后，Consumer 会推进到 Partition 中的下一个 Offset，继续读取消息。 Offset 的推进和记录都是 Consumer 的责任，Kafka 是不管的。

CURENT-OFFSET 就是consumer现在在消息队列中消费到消息的 OFFSET， LOG-END-OFFSET 就是partition中目前最新的消息的OFFSET，LAG是需要消费的消息数

Kafka 中有一个 Consumer Group（消费组）的概念，多个 Consumer 组团去消费一个 Topic。 同组的 Consumer 有相同的 Group ID。 Consumer Group 机制会保障一条消息只被组内唯一一个 Consumer 消费，不会重复消费。 消费组这种方式可以让多个 Partition 并行消费，大大提高了消息的消费能力，最大并行度为 Topic 的 Partition 数量。
    
## Linux常用命令

重要

1. top：查看内存/显示系统当前进程信息
2. df -h：查看磁盘储存状况 
3. iotop：查看IO读写（yum install iotop安装）
4. iotop -o：直接查看比较高的磁盘读写程序
5. netstat -tunlp | grep 端口号：查看端口号占用情况（1）
6. lsof -i:端口号：查看端口号占用情况（2）
7. uptime：查看报告系统运行时长及平均负载
8. ps aux：查看进程

基础

1. 查看目录与文件：ls -la：显示当前目录下所有文件的详细信息
2、切换目录：cd
cd /home 进入 ‘/ home’ 目录
cd … 返回上一级目录
cd …/… 返回上两级目录
3、显示当前目录：pwd
pwd
4、创建空文件：touch
touch desc.txt：在当前目录下创建文件desc.txt
5、创建目录：mkdir
mkdir test：在当前目录下创建test目录
mkdir -p /opt/test/img：在/opt/test目录下创建目录img，若无test目录，先创建test目录
6、查看文件内容：cat
cat desc.txt：查看desc.txt的内容
7、分页查看文件内容：more
more desc.txt：分页查看desc.txt的内容
8、查看文件尾内容：tail
tail -100 desc.txt：查看desc.txt的最后100行内容
9、拷贝：cp
cp desc.txt /mnt/：拷贝desc.txt到/mnt目录下
cp -r test /mnt/：拷贝test目录到/mnt目录下
10、剪切或改名：
mv desc.txt /mnt/：剪切文件desc.txt到目录/mnt下
mv 原名 新名
11、删除：rm
rm -rf test：删除test目录，-r递归删除，-f强制删除。危险操作，务必小心，切记！
12、搜索文件：find
find /opt -name ‘*.txt’：在opt目录下查找以.txt结尾的文件
13、显示或配置网络设备：ifconfig
ifconfig：显示网络设备情况
14、显示网络相关信息：netstat
netstat -a：列出所有端口
netstat -tunlp | grep 端口号：查看进程端口号
15、显示进程状态：ps
ps -ef：显示当前所有进程
ps-ef | grep java：显示当前所有java相关进程
16、查看目录使用情况：du
du -h /opt/test：查看/opt/test目录的磁盘使用情况
17、查看磁盘空间使用情况：df
df -h：查看磁盘空间使用情况
18、显示系统当前进程信息：top
top：显示系统当前进程信息
19、杀死进程：kill
kill -s 9 27810：杀死进程号为27810的进程，强制终止，系统资源无法回收
20、压缩和解压：tar
tar -zcvf test.tar.gz ./test：打包test目录为test.tar.gz文件，-z表示用gzip压缩
tar -zxvf test.tar.gz：解压test.tar.gz文件
21、改变文件或目录的拥有者和组：chown
chown nginx:nginx desc.txt：变更文件desc.txt的拥有者为nginx，用户组为nginx
chown -R nginx:nginx test：变更test及目录下所有文件的拥有者为nginx，用户组为nginx
22、改变文件或目录的访问权限：chmod
chmod u+x test.sh：权限范围：u(拥有者)g(郡组)o(其它用户)， 权限代号：r(读权限/4)w(写权限/2)x(执行权限/1)#给文件拥有者增加test.sh的执行权限
chmod u+x -R test：给文件拥有者增加test目录及其下所有文件的执行权限
23、文本编辑：vim
vim三种模式：命令模式，插入模式，编辑模式。使用ESC或i或：来切换模式。
命令模式下:q退出 :q!强制退出 :wq!保存退出 :set number显示行号 /java在文档中查找java yy复制 p粘贴
vim desc.txt：编辑desc.txt文件
24、关机或重启：shutdown
shutdown -h now：立刻关机
shutdown -r -t 60：60秒后重启
shutdown -r now：重启(1)
reboot：重启(2)
25、帮助命令：man
man ls：查看ls命令的帮助文档
help
## Django
